{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qui-8ZGWn0_O",
    "outputId": "c5923225-7e1d-4710-938d-671d5479abcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 7.0MB 7.2MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -U torchtext==0.8.0 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ES73alZRdnyW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import spacy\n",
    "import pandas as pd\n",
    "# from utils import translate_sentence, bleu, save_checkpoint, load_checkpoint\n",
    "from torchtext.datasets import Multi30k\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rUeFhtSwPrDT",
    "outputId": "d6c3b4bb-eacf-4844-f0c7-a3a540f7c3a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          input_text                                        target_text\n",
      "0  Alvah_Sabin | party | Whig_Party_(United_State...  The largest city in Vermont is Burlington and ...\n",
      "1  Aarhus_Airport | location | Tirstrup && Tirstr...  The location of Aarhus Airport is Tirstrup whi...\n",
      "2  Aaron_Bertram | associatedBand/associatedMusic...  Aaron Bertram, who started performing in 1998 ...\n",
      "3  Abilene,_Texas | isPartOf | Texas && United_St...  Abilene is a part of Texas, in the United Stat...\n",
      "4  Aaron_Turner | associatedBand/associatedMusica...  Aaron Turner played with the bands Twilight an...\n",
      "train :  36079 test : 4009\n"
     ]
    }
   ],
   "source": [
    "#read the train csv file and make a validation file from it, \n",
    "#no need to run it \n",
    "\n",
    "# def make_val():\n",
    "#     df = pd.read_csv('/content/drive/MyDrive/fourth_sem/capstone/t5/data/webNLG2020_train.csv')\n",
    "#     train, val = train_test_split(df, test_size=0.2)\n",
    "#     # print(len(train),len(val))\n",
    "#     train.to_csv('/content/drive/MyDrive/fourth_sem/capstone/t5/data/train.csv')\n",
    "#     val.to_csv('/content/drive/MyDrive/fourth_sem/capstone/t5/data/val.csv')\n",
    "    \n",
    "# make_val()\n",
    "#read train val and test to perform initail cleaning\n",
    "# def clean_df(df):\n",
    "#     # print(df.columns)\n",
    "#     cols = ['prefix','Unnamed: 0','Unnamed: 0.1']\n",
    "#     for col in cols:\n",
    "#         if col in df.columns:\n",
    "#             df = df.drop(col,axis=1)\n",
    "#     # df.rename(columns = {'input_text':'german', 'target_text':'english'}, inplace = True) \n",
    "#     return df\n",
    "\n",
    "# def add_bar(text):\n",
    "#     return text.replace(' ',\" | \")\n",
    "\n",
    "# def mix(train, test):\n",
    "#     df = pd.concat([train, test]).sample(frac=1).reset_index(drop=True)\n",
    "#     train, test = train_test_split(df, test_size=0.1)\n",
    "#     train = train.sample(frac=1).reset_index(drop=True)\n",
    "#     test = test.sample(frac=1).reset_index(drop=True)\n",
    "#     # print(len(train),len(test))\n",
    "#     return train, test\n",
    "\n",
    "# def read_tvt():\n",
    "#     path = '/content/drive/MyDrive/fourth_sem/capstone/t5/data/'\n",
    "#     wine = clean_df(pd.read_csv(path+'wine_triplets_text_uni.csv'))\n",
    "#     wine['input_text'] = wine['input_text'].apply(lambda x: add_bar(x))\n",
    "#     # print(wine.head(),len(wine))\n",
    "#     #reading area and geograpy extracted triplets and sentences\n",
    "#     area_and_geo = clean_df(pd.read_csv(path + 'area_and_geo.csv'))\n",
    "#     train = clean_df(pd.read_csv(path+'webNLG2020_train.csv'))\n",
    "#     #concat wine and area and geo with train dataset\n",
    "#     train = pd.concat([train, wine]).sample(frac=1).reset_index(drop=True)\n",
    "#     train = pd.concat([train, area_and_geo]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#     # val = clean_df(pd.read_csv(path+'val.csv'))\n",
    "#     test = clean_df(pd.read_csv(path+'webNLG2020_dev.csv'))\n",
    "#     train, test = mix(train,test) \n",
    "#     train.to_csv(path+'train_t.csv',index=False)\n",
    "#     # val.to_csv(path+'val_t.csv',index=False)\n",
    "#     test.to_csv(path+'test_t.csv',index=False)\n",
    "#     print(train.head())\n",
    "#     print('train : ',len(train),'test :', len(test))\n",
    "    \n",
    "# read_tvt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MtWhfSGMqpJE",
    "outputId": "0e7bb2c9-d142-4c4b-d649-c5abd475e344"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:52: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n"
     ]
    }
   ],
   "source": [
    "spacy_eng = spacy.load(\"en\")\n",
    "count = 0\n",
    "def tokenize_eng(text):\n",
    "    global count\n",
    "    if count % 1000 == 0:\n",
    "        print(count)\n",
    "    count += 1\n",
    "    return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "\n",
    "input_text = Field(tokenize=tokenize_eng, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "target_text = Field(tokenize=tokenize_eng, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "\n",
    "fields = {'input_text':('src',input_text), 'target_text':('trg',target_text)}\n",
    "# train_data, test_data = TabularDataset.splits(\n",
    "#                                             path = '/content/drive/MyDrive/fourth_sem/capstone/t5/data/',\n",
    "#                                             train = 'train_t.csv',\n",
    "#                                             test = 'test_t.csv',\n",
    "#                                             format = 'csv',\n",
    "#                                             fields=fields)\n",
    "train_data, test_data = TabularDataset.splits(\n",
    "                                            path = '../data/all_merged/',\n",
    "                                            train = 'train.csv',\n",
    "                                            test = 'test.csv',\n",
    "                                            format = 'csv',\n",
    "                                            fields=fields)\n",
    "input_text.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "target_text.build_vocab(train_data, max_size=10000, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZDtlBL2ap7T"
   },
   "outputs": [],
   "source": [
    "# #dump the train and test data\n",
    "# import json\n",
    "# def save_examples(dataset, savepath):\n",
    "#     with open(savepath, 'w') as f:\n",
    "#         # Save num. elements (not really need it)\n",
    "#         f.write(json.dumps(total))  # Write examples length\n",
    "#         f.write(\"\\n\")\n",
    "\n",
    "#         # Save elements\n",
    "#         for pair in dataset.examples:\n",
    "#             data = [pair.src, pair.trg]\n",
    "#             f.write(json.dumps(data))  # Write samples\n",
    "#             f.write(\"\\n\")\n",
    "\n",
    "\n",
    "# def load_examples(filename):\n",
    "#     examples = []\n",
    "#     with open(filename, 'r') as f:\n",
    "#         # Read num. elements (not really need it)\n",
    "#         total = json.loads(f.readline())\n",
    "\n",
    "#         # Save elements\n",
    "#         for i in range(total):\n",
    "#             line = f.readline()\n",
    "#             example = json.loads(line)\n",
    "#             # example = data.Example().fromlist(example, fields)  # Create Example obj. (you can do it here or later)\n",
    "#             examples.append(example)\n",
    "\n",
    "#     end = time.time()\n",
    "#     print(end - start)\n",
    "#     return examples\n",
    "\n",
    "# save_examples(train_data,'/content/drive/MyDrive/fourth_sem/capstone/t5/data/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3JENIc-WkUGP",
    "outputId": "05a37b57-f610-4480-9298-bc60c83e133e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/fourth_sem/capstone/transformers\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/MyDrive/fourth_sem/capstone/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5UX40sIe2zv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "from torchtext.data.metrics import bleu_score\n",
    "import sys\n",
    "\n",
    "\n",
    "def translate_sentence(model, sentence, input_text, target_text, device, max_length=50):\n",
    "    spacy_ger = spacy.load(\"en\") #changed loader from de to en\n",
    "\n",
    "    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n",
    "    if type(sentence) == str:\n",
    "        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    # Add <SOS> and <EOS> in beginning and end respectively\n",
    "    tokens.insert(0, input_text.init_token)\n",
    "    tokens.append(input_text.eos_token)\n",
    "\n",
    "    # Go through each input_text token and convert to an index\n",
    "    text_to_indices = [input_text.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    # Convert to Tensor\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
    "\n",
    "    outputs = [target_text.vocab.stoi[\"<sos>\"]]\n",
    "    for i in range(max_length):\n",
    "        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(sentence_tensor, trg_tensor)\n",
    "\n",
    "        best_guess = output.argmax(2)[-1, :].item()\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        if best_guess == target_text.vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    translated_sentence = [target_text.vocab.itos[idx] for idx in outputs]\n",
    "    # remove start token\n",
    "    return translated_sentence[1:]\n",
    "\n",
    "\n",
    "def bleu(data, model, input_text, target_text, device):\n",
    "    targets = []\n",
    "    outputs = []\n",
    "\n",
    "    for example in data:\n",
    "        src = vars(example)[\"src\"]\n",
    "        trg = vars(example)[\"trg\"]\n",
    "\n",
    "        prediction = translate_sentence(model, src, input_text, target_text, device)\n",
    "        prediction = prediction[:-1]  # remove <eos> token\n",
    "\n",
    "        targets.append([trg])\n",
    "        outputs.append(prediction)\n",
    "\n",
    "    return bleu_score(outputs, targets)\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQkLgGEeSuPc"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_len,\n",
    "        device,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
    "        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n",
    "        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "\n",
    "        self.device = device\n",
    "        self.transformer = nn.Transformer(\n",
    "            embedding_size,\n",
    "            num_heads,\n",
    "            num_encoder_layers,\n",
    "            num_decoder_layers,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = src.transpose(0, 1) == self.src_pad_idx\n",
    "\n",
    "        # (N, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_seq_length, N = src.shape\n",
    "        trg_seq_length, N = trg.shape\n",
    "\n",
    "        src_positions = (\n",
    "            torch.arange(0, src_seq_length)\n",
    "            .unsqueeze(1)\n",
    "            .expand(src_seq_length, N)\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        trg_positions = (\n",
    "            torch.arange(0, trg_seq_length)\n",
    "            .unsqueeze(1)\n",
    "            .expand(trg_seq_length, N)\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        embed_src = self.dropout(\n",
    "            (self.src_word_embedding(src) + self.src_position_embedding(src_positions))\n",
    "        )\n",
    "        embed_trg = self.dropout(\n",
    "            (self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions))\n",
    "        )\n",
    "\n",
    "        src_padding_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        out = self.transformer(\n",
    "            embed_src,\n",
    "            embed_trg,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_mask=trg_mask,\n",
    "        )\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s9Hdtmu5dc22",
    "outputId": "7ac288b1-b40b-429f-acd7-594baa5a8d11"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# We're ready to define everything we need for training our Seq2Seq model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "load_model = False\n",
    "save_model = False\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 11\n",
    "learning_rate = 3e-4\n",
    "batch_size = 8\n",
    "\n",
    "# Model hyperparameters\n",
    "src_vocab_size = len(input_text.vocab)\n",
    "trg_vocab_size = len(target_text.vocab)\n",
    "embedding_size = 400\n",
    "num_heads = 8\n",
    "num_encoder_layers = 3 \n",
    "num_decoder_layers = 3\n",
    "dropout = 0.10\n",
    "max_len = 150\n",
    "forward_expansion = 4\n",
    "src_pad_idx = target_text.vocab.stoi[\"<pad>\"]\n",
    "\n",
    "# # Tensorboard to get nice loss plot\n",
    "# writer = SummaryWriter(\"runs/loss_plot\")\n",
    "# step = 0\n",
    "\n",
    "train_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data,test_data),\n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    device=device,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMOpo5rZ3agf"
   },
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    embedding_size,\n",
    "    src_vocab_size,\n",
    "    trg_vocab_size,\n",
    "    src_pad_idx,\n",
    "    num_heads,\n",
    "    num_encoder_layers,\n",
    "    num_decoder_layers,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    max_len,\n",
    "    device,\n",
    "    ).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)        \n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.1, patience=10, verbose=True\n",
    ")\n",
    "\n",
    "pad_idx = target_text.vocab.stoi[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6_NUGfsE2wBz",
    "outputId": "12ff2eeb-6202-4da0-ea40-cbef5bd4bf44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 / 11]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['1974', 'schiphol', 'only', 'only', 'javanese', 'helped', 'schiphol', 'schiphol', 'tze', 'tze', 'tze', '1411.0', '1411.0', '1411.0', '253260', 'alter', 'only', 'romanesque', '.', 'alter', 'marjorie', 'schiphol', 'schiphol', '.0068', 'only', 'systems', 'limerick', 'developers', '103', 'brandon', 'only', '3800.0', 'ct', 'scrapped', 'only', 'schiphol', 'schiphol', 'only', '16244700.0', '253260', 'santilli', 'only', 'only', '1411.0', 'tze', 'therefore', 'gasparis', 'acura', 'schiphol', 'african', 'schiphol', 'tze', 'therefore', 'african', 'tze', 'tze', 'tze', 'tze', 'electronic', 'drastically', 'only', 'virginia', '1411.0', 'only', 'representative', 'on', 'gasparis', 'tze', 'jamali', 'on', '253260', '253260', 'elizabeth', 'developers', 'electronic', 'gasparis', '1411.0', '253260', 'rovers', 'african', 'schiphol', 'african', 'schiphol', 'african', 'only', 'african', 'african', 'only', 'brandon', 'only', 'african', 'african', 'african', 'size', '253260', 'only', 'only', 'alter', 'only', 'only']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch loss  2.0104990005493164\n",
      "[Epoch 1 / 11]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['american', 'alan', 'bean', ',', 'who', 'graduated', 'from', 'the', 'university', 'of', 'texas', 'in', '1955', ',', 'graduated', 'from', 'ut', 'austin', 'with', 'a', 'b.s.', 'in', '1955', '.', 'he', 'graduated', 'from', 'the', 'university', 'of', 'texas', 'in', '1955', 'with', 'a', 'bachelor', 'of', 'science', 'degree', 'in', '1955', '.', '<eos>']\n",
      "epoch loss  0.7371901869773865\n",
      "[Epoch 2 / 11]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['alan', 'bean', 'was', 'a', 'test', 'pilot', 'who', 'was', 'born', 'in', 'wheeler', ',', 'texas', '.', 'he', 'graduated', 'from', 'ut', 'austin', 'in', '1955', 'with', 'a', 'b.s.', 'in', '1955', 'with', 'a', 'b.s.', 'in', '1963', 'with', 'a', 'b.s.', 'he', 'was', 'a', 'test', 'pilot', '.', '<eos>']\n",
      "epoch loss  1.488113522529602\n",
      "[Epoch 3 / 11]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['alan', 'bean', 'was', 'born', 'in', 'wheeler', ',', 'texas', 'and', 'graduated', 'from', 'ut', 'austin', 'in', '1955', 'with', 'a', 'b.s.', 'he', 'served', 'as', 'a', 'test', 'pilot', 'and', 'was', 'a', 'crew', 'member', 'of', 'apollo', '12', '.', 'he', 'was', 'a', 'crew', 'member', 'of', 'apollo', '12', '.', '<eos>']\n",
      "epoch loss  0.9800057411193848\n",
      "[Epoch 4 / 11]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['alan', 'bean', 'was', 'born', 'in', 'wheeler', ',', 'texas', ',', 'usa', 'and', 'graduated', 'from', 'ut', 'austin', 'in', '1955', 'with', 'a', 'b.s.', 'he', 'was', 'a', 'test', 'pilot', '.', 'he', 'was', 'a', 'test', 'pilot', 'and', 'he', 'was', 'a', 'test', 'pilot', '.', '<eos>']\n",
      "epoch loss  1.2569204568862915\n",
      "[Epoch 5 / 11]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['alan', 'bean', 'was', 'a', 'us', 'national', 'who', 'was', 'born', 'in', 'wheeler', ',', 'texas', '.', 'he', 'graduated', 'from', 'ut', 'austin', 'in', '1955', 'with', 'a', 'b.s.', 'he', 'became', 'a', 'test', 'pilot', 'and', 'became', 'a', 'member', 'of', 'the', 'apollo', '12', 'crew', '.', 'he', 'spent', '100305', 'minutes', 'in', 'space', '.', '<eos>']\n",
      "epoch loss  0.7588708400726318\n",
      "[Epoch 6 / 11]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['american', 'alan', 'bean', 'was', 'born', 'in', 'wheeler', ',', 'texas', 'and', 'graduated', 'from', 'ut', 'austin', 'in', '1955', 'with', 'a', 'b.s.', 'he', 'was', 'a', 'test', 'pilot', 'and', 'is', 'now', 'retired', '.', 'he', 'was', 'a', 'crew', 'member', 'of', 'apollo', '12', '.', '<eos>']\n",
      "epoch loss  1.1227976083755493\n",
      "[Epoch 7 / 11]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['american', 'born', 'alan', 'bean', 'was', 'a', 'test', 'pilot', 'who', 'was', 'a', 'crew', 'member', 'of', 'apollo', '12', '.', 'he', 'was', 'a', 'test', 'pilot', 'who', 'was', 'a', 'crew', 'member', 'of', 'apollo', '12', 'and', 'spent', '100305', 'minutes', 'in', 'space', '.', '<eos>']\n",
      "epoch loss  0.6923632621765137\n",
      "[Epoch 8 / 11]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['alan', 'bean', 'is', 'an', 'american', 'who', 'was', 'born', 'in', 'wheeler', ',', 'texas', '.', 'he', 'graduated', 'from', 'ut', 'austin', 'in', '1955', 'with', 'a', 'bachelor', 'of', 'science', 'degree', ',', 'and', 'served', 'as', 'a', 'test', 'pilot', 'before', 'being', 'a', 'crew', 'member', 'of', 'apollo', '12', '.', 'he', 'spent', '100305', 'minutes', 'in', 'space', '.', '<eos>']\n",
      "epoch loss  0.6366934180259705\n",
      "[Epoch 9 / 11]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['alan', 'bean', 'is', 'an', 'american', 'who', 'was', 'born', 'in', 'wheeler', ',', 'texas', '.', 'he', 'graduated', 'from', 'ut', 'austin', 'in', '1955', 'with', 'a', 'bs', 'of', 'the', 'united', 'states', '.', 'he', 'was', 'a', 'test', 'pilot', 'who', 'was', 'a', 'crew', 'member', 'of', 'apollo', '12', '.', '<eos>']\n",
      "epoch loss  0.987153172492981\n",
      "[Epoch 10 / 11]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['alan', 'bean', 'was', 'born', 'in', 'wheeler', ',', 'texas', 'in', '1932', 'and', 'graduated', 'from', 'ut', 'austin', 'in', '1955', 'with', 'a', 'b.s.', 'degree', '.', 'he', 'was', 'a', 'test', 'pilot', 'and', 'became', 'a', 'member', 'of', 'the', 'apollo', '12', 'crew', '.', 'he', 'spent', 'in', 'june', ',', 'b.s.', 'degree', ',', 'b.s.', 'degree', ',', 'degree', 'degree', ',', 'and', 'is', 'now', 'retired', '.', '<eos>']\n",
      "epoch loss  1.0500013828277588\n"
     ]
    }
   ],
   "source": [
    "model_name = str(num_epochs) + str(batch_size) + str(embedding_size) + str(num_heads) + str(num_encoder_layers) + str(num_decoder_layers) + '.pth.tar'\n",
    "# num_epochs = 11\n",
    "# batch_size = 8\n",
    "# embedding_size = 400\n",
    "# num_heads = 8\n",
    "# num_encoder_layers = 3 \n",
    "# num_decoder_layers = 3\n",
    "if load_model:\n",
    "    # load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
    "    load_checkpoint(torch.load(mode_name), model, optimizer)\n",
    "\n",
    "sentence = \"Alan_Bean | nationality | United_States && Alan_Bean | occupation | Test_pilot && Alan_Bean | almaMater | UT Austin, B.S. 1955 && Alan_Bean | birthPlace | Wheeler,_Texas && Alan_Bean | timeInSpace | 100305.0 && Alan_Bean | selection | 1963 && Alan_Bean | status | Retired\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "\n",
    "    if save_model:\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "        }\n",
    "        save_checkpoint(checkpoint,model_name)\n",
    "\n",
    "    model.eval()\n",
    "    translated_sentence = translate_sentence(\n",
    "        model, sentence, input_text, target_text, device, max_length=100\n",
    "    )\n",
    "\n",
    "    print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        # Get input and targets and get to cuda\n",
    "        inp_data = batch.src.to(device)\n",
    "        target = batch.trg.to(device)\n",
    "        print(inp_data.shape,target.shape)\n",
    "\n",
    "        # Forward prop\n",
    "        output = model(inp_data, target[:-1,:])\n",
    "\n",
    "        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n",
    "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
    "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
    "        # way that we have output_words * batch_size that we want to send in into\n",
    "        # our cost function, so we need to do some reshapin.\n",
    "        # Let's also remove the start token while we're at it\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
    "        # within a healthy range\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        # plot to tensorboard\n",
    "        # writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "        # step += 1\n",
    "    print(\"epoch loss \",losses[-1])\n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    scheduler.step(mean_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JXU-Orwtt1qo",
    "outputId": "656bf1eb-1836-4da3-98ba-f8286b54062b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['whitehall', 'lane', 'cabernet', 'franc', 'has', 'medium', 'body', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "# load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
    "sentence = 'WhitehallLaneCabernetFranc | hasBody | medium'\n",
    "translated_sentence = translate_sentence(\n",
    "        model, sentence, input_text, target_text, device, max_length=100)\n",
    "print(translated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uoRFXHSPwhu5",
    "outputId": "08674215-8064-4024-a193-54026042a84e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu score 33.87\n"
     ]
    }
   ],
   "source": [
    "# running on entire test data takes a while\n",
    "score = bleu(test_data[0:500], model, input_text, target_text, device)\n",
    "print(f\"Bleu score {score * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLaM9RAdKuwh",
    "outputId": "23f309e1-3f0f-4ce0-ce57-af5eac974809"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu score 33.50\n"
     ]
    }
   ],
   "source": [
    "score = bleu(test_data[500:1000], model, input_text, target_text, device)\n",
    "print(f\"Bleu score {score * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ugjKWx_EyZ_u",
    "outputId": "e9ca2dfa-68b8-4d26-c5b6-158f59695c79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu score 34.28\n"
     ]
    }
   ],
   "source": [
    "score = bleu(test_data[1000:1500], model, input_text, target_text, device)\n",
    "print(f\"Bleu score {score * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qn0m-kJqtGCU",
    "outputId": "ba8c78fc-e527-4448-dfd7-c3d2d3acec8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu score 32.34\n"
     ]
    }
   ],
   "source": [
    "score = bleu(test_data[1500:2000], model, input_text, target_text, device)\n",
    "print(f\"Bleu score {score * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yCWiPEgDtJbI",
    "outputId": "ed9e2086-d5c0-493e-bc0f-4fca8811b9d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu score 34.06\n"
     ]
    }
   ],
   "source": [
    "score = bleu(test_data[2000:2500], model, input_text, target_text, device)\n",
    "print(f\"Bleu score {score * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hj7DlZwztMD2",
    "outputId": "70b4fcb2-d7ba-4e00-a87c-bb8116058c9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu score 32.54\n"
     ]
    }
   ],
   "source": [
    "score = bleu(test_data[2500:3000], model, input_text, target_text, device)\n",
    "print(f\"Bleu score {score * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "I28KdwtrtPNK",
    "outputId": "59f2f41c-8589-4980-9a5f-f41a4f13b7a8"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a8c549930cd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Bleu score {score * 100:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-e5630ff96436>\u001b[0m in \u001b[0;36mbleu\u001b[0;34m(data, model, input_text, target_text, device)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbleu_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/data/metrics.py\u001b[0m in \u001b[0;36mbleu_score\u001b[0;34m(candidate_corpus, references_corpus, max_n, weights)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mngram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandidate_counter\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# TODO: no need to loop through the whole counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mtotal_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcandidate_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_counts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for dimension 0 with size 4"
     ]
    }
   ],
   "source": [
    "score = bleu(test_data[3000:3500], model, input_text, target_text, device)\n",
    "print(f\"Bleu score {score * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FkslNPyutRiQ"
   },
   "outputs": [],
   "source": [
    "score = bleu(test_data[3500:4000], model, input_text, target_text, device)\n",
    "print(f\"Bleu score {score * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sIHFjdu2tXdu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-8AxtvXyj9A"
   },
   "outputs": [],
   "source": [
    "# for sent in test_data[500:1000]:\n",
    "#     print(sent.src,sent.trg)\n",
    "#     translated_sentence = translate_sentence(model, sent.src, german, english, device, max_length=100)\n",
    "#     print(translated_sentence)\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transformer_triplet_to_text.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
